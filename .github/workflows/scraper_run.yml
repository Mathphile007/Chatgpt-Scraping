name: ChatGPT Scraper Run

# 1. Trigger the workflow manually from the GitHub UI
on:
  workflow_dispatch:

jobs:
  scrape-and-save:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Download your repo code
      - name: Checkout Repository Code
        uses: actions/checkout@v4

      # Step 2: Set up Python 3.11 (A stable version for scraping)
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Step 3: Install Dependencies (SeleniumBase and Pandas)
      - name: Install SeleniumBase and Dependencies (FORCE UPGRADE)
        run: |
          python -m pip install --upgrade pip
          # Force a complete reinstall of the latest SeleniumBase package
          pip install -U seleniumbase --force-reinstall
          # Install undetected-chromedriver separately just in case the SB install is incomplete
          pip install undetected-chromedriver
          # Install the driver explicitly
          seleniumbase install uc_driver
      # Step 4: Run the Scraper Script
      - name: Execute ChatGPT Scraper
        # The 'run' keyword executes the file we just created
        run: python chatgpt_scraper.py
        
      # Step 5: Upload the final JSON file as an Artifact (CRUCIAL!)
      - name: Upload Scraped Output
        uses: actions/upload-artifact@v4
        with:
          name: chatgpt-scraped-data
          # This must match the filename created in the Python script!
          path: chatgpt_output.json 
          retention-days: 7 # Keep the artifact for 7 days
